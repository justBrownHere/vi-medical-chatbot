{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-07T16:18:34.150894Z",
     "iopub.status.busy": "2026-01-07T16:18:34.150632Z",
     "iopub.status.idle": "2026-01-07T16:21:09.446294Z",
     "shell.execute_reply": "2026-01-07T16:21:09.445173Z",
     "shell.execute_reply.started": "2026-01-07T16:18:34.150869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\n",
      "/bin/bash: line 1: conda: command not found\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m118.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m45.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m440.9/440.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for vllm-nccl-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.3.0 which is incompatible.\n",
      "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!conda create -n myenv python=3.10 -y\n",
    "!conda activate myenv\n",
    "\n",
    "# Install vLLM with CUDA 12.1.\n",
    "!pip install vllm==0.4.2 -q\n",
    "# !pip install -U vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:21:14.325409Z",
     "iopub.status.busy": "2026-01-07T16:21:14.324823Z",
     "iopub.status.idle": "2026-01-07T16:21:58.560174Z",
     "shell.execute_reply": "2026-01-07T16:21:58.559364Z",
     "shell.execute_reply.started": "2026-01-07T16:21:14.325351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.0/417.0 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.5/664.5 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\n",
      "distributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\n",
      "pandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -qq install langchain==0.1.20 optimum==1.19.2 qdrant-client==1.9.1 wikipedia fastapi==0.111.0 uvicorn==0.29.0 pyngrok==7.1.6 langchain-google-genai==1.0.3 google-generativeai==0.5.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:22:08.933089Z",
     "iopub.status.busy": "2026-01-07T16:22:08.932769Z",
     "iopub.status.idle": "2026-01-07T16:22:09.090950Z",
     "shell.execute_reply": "2026-01-07T16:22:09.090321Z",
     "shell.execute_reply.started": "2026-01-07T16:22:08.933064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "GENERATE_MODEL_NAME=\"brown1808/vinallama-7b-finetune-v17\"\n",
    "EMBEDDINGS_MODEL_NAME=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "QDRANT_URL = \"\"\n",
    "QDRANT_COLLECTION_NAME = \"disease_corpus_vector_db_v2\"\n",
    "NGROK_STATIC_DOMAIN = \"ruling-plainly-jaguar.ngrok-free.app\"\n",
    "NGROK_TOKEN = \"\"\n",
    "HUGGINGFACE_API_KEY = \"\"\n",
    "QDRANT_API_KEY = \"\"\n",
    "GEMINI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:22:11.810423Z",
     "iopub.status.busy": "2026-01-07T16:22:11.810135Z",
     "iopub.status.idle": "2026-01-07T16:22:19.819295Z",
     "shell.execute_reply": "2026-01-07T16:22:19.818479Z",
     "shell.execute_reply.started": "2026-01-07T16:22:11.810401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KhÃ´ng liÃªn quan\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def classify_question_with_gemini(question):\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=0, \n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_output_tokens=50,\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemma-3-27b-it\", \n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    prompt_text = (\n",
    "        \"Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n loáº¡i vÄƒn báº£n. HÃ£y phÃ¢n loáº¡i cÃ¢u há»i sau thÃ nh 1 trong 3 nhÃ³m:\\n\"\n",
    "        \"NhÃ£n: 'Y táº¿', 'ChÃ o há»i', 'KhÃ´ng liÃªn quan'.\\n\"\n",
    "        f\"CÃ¢u há»i: {question}\\n\"\n",
    "        \"Tráº£ lá»i duy nháº¥t tÃªn nhÃ£n.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt_text)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Lá»—i: {str(e)}\"\n",
    "\n",
    "def generate_chat_title_with_gemini(question):\n",
    "    \"\"\"\n",
    "    Táº¡o tiÃªu Ä‘á» ngáº¯n gá»n (5-7 tá»«) cho Ä‘oáº¡n chat dá»±a trÃªn cÃ¢u há»i Ä‘áº§u tiÃªn\n",
    "    \"\"\"\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        top_k=40,\n",
    "        max_output_tokens=20,\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemma-3-27b-it\", \n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    prompt_text = (\n",
    "        \"Báº¡n lÃ  trá»£ lÃ½ táº¡o tiÃªu Ä‘á» cho cuá»™c trÃ² chuyá»‡n y táº¿.\\n\"\n",
    "        \"NHIá»†M Vá»¤: Táº¡o tiÃªu Ä‘á» NGáº®N Gá»ŒN (5-7 tá»«) cho cÃ¢u há»i sau.\\n\"\n",
    "        \"QUY Táº®C:\\n\"\n",
    "        \"- Chá»‰ tráº£ lá»i tiÃªu Ä‘á», KHÃ”NG thÃªm giáº£i thÃ­ch\\n\"\n",
    "        \"- DÃ¹ng tá»« ngá»¯ sÃºc tÃ­ch, dá»… hiá»ƒu\\n\"\n",
    "        \"- Giá»¯ nguyÃªn nghÄ©a cÃ¢u há»i gá»‘c\\n\"\n",
    "        \"- KhÃ´ng dÃ¹ng dáº¥u ngoáº·c kÃ©p hoáº·c kÃ½ tá»± Ä‘áº·c biá»‡t\\n\\n\"\n",
    "        f\"CÃ¢u há»i: {question}\\n\\n\"\n",
    "        \"TiÃªu Ä‘á» ngáº¯n gá»n:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt_text)\n",
    "        title = response.text.strip()\n",
    "        \n",
    "        # Clean up title\n",
    "        title = title.replace('\"', '').replace(\"'\", '').strip()\n",
    "        \n",
    "        # Náº¿u quÃ¡ dÃ i, cáº¯t ngáº¯n\n",
    "        if len(title) > 50:\n",
    "            title = title[:47] + \"...\"\n",
    "            \n",
    "        return title\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating title: {e}\")\n",
    "        # Fallback: Láº¥y 6 tá»« Ä‘áº§u\n",
    "        words = question.split()[:6]\n",
    "        return \" \".join(words) + (\"...\" if len(question.split()) > 6 else \"\")\n",
    "\n",
    "test = \"HÃ´m nay lÃ  ngÃ y máº¥y?\"\n",
    "print(classify_question_with_gemini(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T16:22:23.892396Z",
     "iopub.status.busy": "2026-01-07T16:22:23.892117Z",
     "iopub.status.idle": "2026-01-07T16:22:37.457167Z",
     "shell.execute_reply": "2026-01-07T16:22:37.456355Z",
     "shell.execute_reply.started": "2026-01-07T16:22:23.892375Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179ef77e6dc4df0aa62dea29b3940d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/814 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9885d0124d2940fc8fa43089c28d13eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bdeacb392c4612b3d34351b734c74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973aed73c80342cb9eba3c71aa1ac989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa3476adecb40289b1518fd2ec6922a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee7fa94c4b34aed9aeaa4f1fc89ef8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_rerank = AutoModelForSequenceClassification.from_pretrained('itdainb/PhoRanker').to(device)\n",
    "model_rerank = BetterTransformer.transform(model_rerank)\n",
    "tokenizer_rerank = AutoTokenizer.from_pretrained('itdainb/PhoRanker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:52:55.520893Z",
     "iopub.status.busy": "2026-01-06T19:52:55.520423Z",
     "iopub.status.idle": "2026-01-06T19:52:56.827415Z",
     "shell.execute_reply": "2026-01-06T19:52:56.826526Z",
     "shell.execute_reply.started": "2026-01-06T19:52:55.520871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from typing import List\n",
    "\n",
    "class RerankRetriever(VectorStoreRetriever):\n",
    "    vectorstore: VectorStoreRetriever\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "        print(f\"docs len: {len(docs)}\")\n",
    "        candidates = [doc.page_content for doc in docs]\n",
    "        queries = [query]*len(candidates)\n",
    "        features = tokenizer_rerank(queries, candidates, padding=True, truncation=True,return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model_rerank(**features).logits\n",
    "            values, indices = torch.sum(scores, dim=1).sort()\n",
    "            # relevant_docs = docs[indices[0]]\n",
    "        return [docs[indices[0]],docs[indices[1]]]\n",
    "    \n",
    "class RerankWikiRetriever(VectorStoreRetriever):\n",
    "    vectorstore: WikipediaRetriever\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = self.vectorstore.get_relevant_documents(query=query)\n",
    "        candidates = [doc.page_content for doc in docs]\n",
    "        queries = [query]*len(candidates)\n",
    "        features = tokenizer_rerank(queries, candidates,  padding=True, truncation=True,return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model_rerank(**features).logits\n",
    "            values, indices = torch.sum(scores, dim=1).sort()\n",
    "            # relevant_docs = docs[indices[0]]\n",
    "        return [docs[indices[0]],docs[indices[1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:52:56.828999Z",
     "iopub.status.busy": "2026-01-06T19:52:56.828339Z",
     "iopub.status.idle": "2026-01-06T19:52:56.857264Z",
     "shell.execute_reply": "2026-01-06T19:52:56.856661Z",
     "shell.execute_reply.started": "2026-01-06T19:52:56.828960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory  \n",
    "\n",
    "class CustomConversationBufferMemory(ConversationBufferWindowMemory):\n",
    "    def save_context(self, inputs, outputs):\n",
    "        if \"source_documents\" in outputs:\n",
    "            outputs.pop(\"source_documents\")\n",
    "        super().save_context(inputs, outputs)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.chat_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:52:56.858341Z",
     "iopub.status.busy": "2026-01-06T19:52:56.858042Z",
     "iopub.status.idle": "2026-01-06T19:52:56.864553Z",
     "shell.execute_reply": "2026-01-06T19:52:56.863585Z",
     "shell.execute_reply.started": "2026-01-06T19:52:56.858310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def rewrite_question_with_gemini(question, chat_history):\n",
    "    generation_config = genai.GenerationConfig(\n",
    "        temperature=0, \n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        max_output_tokens=200,\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemma-3-27b-it\", \n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    # Kiá»ƒm tra liÃªn quan\n",
    "    check_relevance_prompt = (\n",
    "        \"HÃ£y kiá»ƒm tra xem cÃ¢u há»i má»›i cÃ³ liÃªn quan Ä‘áº¿n lá»‹ch sá»­ trÃ² chuyá»‡n khÃ´ng. \"\n",
    "        \"Tráº£ lá»i CHá»ˆ 'true' náº¿u cÃ³ liÃªn quan hoáº·c 'false' náº¿u khÃ´ng liÃªn quan.\\n\\n\"\n",
    "        f\"Lá»‹ch sá»­ trÃ² chuyá»‡n:\\n{chat_history}\\n\\n\"\n",
    "        f\"CÃ¢u há»i má»›i: {question}\\n\\n\"\n",
    "        \"Tráº£ lá»i (true/false):\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response_check = model.generate_content(check_relevance_prompt)\n",
    "        is_related = response_check.text.strip().lower() == \"true\"\n",
    "    except Exception as e:\n",
    "        is_related = False\n",
    "    \n",
    "    # Viáº¿t láº¡i cÃ¢u há»i\n",
    "    if not is_related or not chat_history:\n",
    "        return question\n",
    "\n",
    "    else:\n",
    "        rewrite_prompt = (\n",
    "            \"Nhiá»‡m vá»¥: Káº¿t há»£p thÃ´ng tin tá»« lá»‹ch sá»­ Ä‘á»ƒ lÃ m rÃµ cÃ¢u há»i má»›i.\\n\\n\"\n",
    "            \"QUY Táº®C QUAN TRá»ŒNG:\\n\"\n",
    "            \"- KHÃ”NG thay Ä‘á»•i chá»§ ngá»¯ (giá»¯ nguyÃªn 'tÃ´i', 'mÃ¬nh', 'con tÃ´i', v.v.)\\n\"\n",
    "            \"- KHÃ”NG thÃªm cÃ¢u há»i xÃ¡c nháº­n ('Ä‘Ãºng khÃ´ng?', 'pháº£i khÃ´ng?')\\n\"\n",
    "            \"- CHá»ˆ bá»• sung thÃ´ng tin cáº§n thiáº¿t tá»« lá»‹ch sá»­\\n\"\n",
    "            \"- Giá»¯ nguyÃªn Ã½ Ä‘á»‹nh cá»§a ngÆ°á»i há»i\\n\\n\"\n",
    "            f\"Lá»‹ch sá»­ trÃ² chuyá»‡n:\\n{chat_history}\\n\\n\"\n",
    "            f\"CÃ¢u há»i má»›i: {question}\\n\\n\"\n",
    "            \"Viáº¿t láº¡i (giá»¯ nguyÃªn chá»§ ngá»¯ vÃ  giá»ng Ä‘iá»‡u):\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        response_rewrite = model.generate_content(rewrite_prompt)\n",
    "        return response_rewrite.text.strip()\n",
    "    except Exception as e:\n",
    "        return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:52:56.865783Z",
     "iopub.status.busy": "2026-01-06T19:52:56.865460Z",
     "iopub.status.idle": "2026-01-06T19:52:58.020958Z",
     "shell.execute_reply": "2026-01-06T19:52:58.020011Z",
     "shell.execute_reply.started": "2026-01-06T19:52:56.865739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import VLLM\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class LLMServe:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing LLM Service...\")\n",
    "        self.embeddings = self.load_embeddings()\n",
    "        self.retrievers = {}\n",
    "        self.pipe = self.load_model_pipeline(max_new_tokens=200)\n",
    "        self.prompt = self.load_prompt_template()\n",
    "        \n",
    "        # Session-based memories\n",
    "        self.sessions = {}  # {session_id: {\"memory\": ..., \"source\": ..., \"last_access\": ...}}\n",
    "        self.session_ttl = timedelta(hours=2)  # Session expires after 2 hours\n",
    "        \n",
    "        self.chains = {}\n",
    "        print(\"LLM Service initialized successfully!\")\n",
    "    \n",
    "    def get_session_key(self, session_id, source):\n",
    "        \"\"\"Táº¡o unique key cho session + source\"\"\"\n",
    "        return f\"{session_id}_{source}\"\n",
    "    \n",
    "    def get_or_create_chain(self, session_id, source):\n",
    "        \"\"\"Get hoáº·c táº¡o chain cho session + source\"\"\"\n",
    "        session_key = self.get_session_key(session_id, source)\n",
    "        \n",
    "        # Cleanup expired sessions\n",
    "        self._cleanup_expired_sessions()\n",
    "        \n",
    "        if session_key not in self.chains:\n",
    "            print(f\"Creating new chain for session: {session_key}\")\n",
    "            retriever = self.load_retriever(source, self.embeddings)\n",
    "            memory = CustomConversationBufferMemory(\n",
    "                memory_key=\"chat_history\", \n",
    "                return_messages=True,\n",
    "                k=3  # Chá»‰ giá»¯ 3 turns gáº§n nháº¥t Ä‘á»ƒ trÃ¡nh context bleeding\n",
    "            )\n",
    "            \n",
    "            if source not in self.retrievers:\n",
    "                self.retrievers[source] = retriever\n",
    "            \n",
    "            self.sessions[session_key] = {\n",
    "                \"memory\": memory,\n",
    "                \"source\": source,\n",
    "                \"session_id\": session_id,\n",
    "                \"created_at\": datetime.now(),\n",
    "                \"last_access\": datetime.now()\n",
    "            }\n",
    "            \n",
    "            self.chains[session_key] = self.load_rag_pipeline_with_memory(\n",
    "                llm=self.pipe,\n",
    "                retriever=retriever,\n",
    "                prompt=self.prompt,\n",
    "                memory=memory\n",
    "            )\n",
    "        else:\n",
    "            # Update last access time\n",
    "            self.sessions[session_key][\"last_access\"] = datetime.now()\n",
    "        \n",
    "        return self.chains[session_key]\n",
    "    \n",
    "    def clear_session(self, session_id, source=None):\n",
    "        \"\"\"Clear memory cho má»™t session cá»¥ thá»ƒ\"\"\"\n",
    "        if source:\n",
    "            # Clear specific source\n",
    "            session_key = self.get_session_key(session_id, source)\n",
    "            if session_key in self.sessions:\n",
    "                self.sessions[session_key][\"memory\"].clear()\n",
    "                del self.sessions[session_key]\n",
    "                if session_key in self.chains:\n",
    "                    del self.chains[session_key]\n",
    "                return f\"Cleared session {session_key}\"\n",
    "        else:\n",
    "            # Clear all sources for this session\n",
    "            keys_to_delete = [\n",
    "                key for key in self.sessions.keys() \n",
    "                if key.startswith(f\"{session_id}_\")\n",
    "            ]\n",
    "            for key in keys_to_delete:\n",
    "                self.sessions[key][\"memory\"].clear()\n",
    "                del self.sessions[key]\n",
    "                if key in self.chains:\n",
    "                    del self.chains[key]\n",
    "            return f\"Cleared {len(keys_to_delete)} sessions for {session_id}\"\n",
    "    \n",
    "    def load_session_memory(self, session_id, source, chat_history):\n",
    "        \"\"\"\n",
    "        Load lá»‹ch sá»­ chat vÃ o memory khi user quay láº¡i chat cÅ©\n",
    "        \n",
    "        Args:\n",
    "            session_id: ID cá»§a chat\n",
    "            source: vi-medical hoáº·c wiki\n",
    "            chat_history: Array cá»§a {question, answer} tá»« frontend\n",
    "        \"\"\"\n",
    "        session_key = self.get_session_key(session_id, source)\n",
    "        \n",
    "        # Clear existing memory\n",
    "        if session_key in self.sessions:\n",
    "            self.sessions[session_key][\"memory\"].clear()\n",
    "        \n",
    "        # Get or create chain\n",
    "        chain = self.get_or_create_chain(session_id, source)\n",
    "        memory = self.sessions[session_key][\"memory\"]\n",
    "        \n",
    "        # Load history vÃ o memory\n",
    "        for turn in chat_history:\n",
    "            question = turn.get(\"question\", \"\")\n",
    "            answer = turn.get(\"answer\", \"\")\n",
    "            if question and answer:\n",
    "                memory.save_context(\n",
    "                    {\"question\": question},\n",
    "                    {\"answer\": answer}\n",
    "                )\n",
    "        \n",
    "        print(f\"Loaded {len(chat_history)} turns into session {session_key}\")\n",
    "        return f\"Loaded {len(chat_history)} conversation turns\"\n",
    "    \n",
    "    def _cleanup_expired_sessions(self):\n",
    "        \"\"\"XÃ³a sessions Ä‘Ã£ háº¿t háº¡n\"\"\"\n",
    "        now = datetime.now()\n",
    "        expired_keys = [\n",
    "            key for key, session in self.sessions.items()\n",
    "            if now - session[\"last_access\"] > self.session_ttl\n",
    "        ]\n",
    "        \n",
    "        for key in expired_keys:\n",
    "            print(f\"Cleaning up expired session: {key}\")\n",
    "            if key in self.sessions:\n",
    "                del self.sessions[key]\n",
    "            if key in self.chains:\n",
    "                del self.chains[key]\n",
    "        \n",
    "        if expired_keys:\n",
    "            print(f\"Cleaned up {len(expired_keys)} expired sessions\")\n",
    "    \n",
    "    def detect_topic_change(self, new_question, session_key):\n",
    "        \"\"\"\n",
    "        PhÃ¡t hiá»‡n náº¿u cÃ¢u há»i má»›i chuyá»ƒn topic y táº¿ hoÃ n toÃ n khÃ¡c\n",
    "        Sá»­ dá»¥ng Gemini gemma-3-27b-it Ä‘á»ƒ phÃ¢n tÃ­ch\n",
    "        \"\"\"\n",
    "        if session_key not in self.sessions:\n",
    "            return False\n",
    "        \n",
    "        memory = self.sessions[session_key][\"memory\"]\n",
    "        \n",
    "        try:\n",
    "            history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        # Cáº§n Ã­t nháº¥t 2 messages Ä‘á»ƒ so sÃ¡nh\n",
    "        if len(history) < 2:\n",
    "            return False\n",
    "        \n",
    "        # Láº¥y cÃ¢u há»i gáº§n nháº¥t tá»« history\n",
    "        recent_question = None\n",
    "        for msg in reversed(history):\n",
    "            if hasattr(msg, 'content') and hasattr(msg, 'type'):\n",
    "                if msg.type == 'human':\n",
    "                    recent_question = msg.content\n",
    "                    break\n",
    "        \n",
    "        if not recent_question:\n",
    "            return False\n",
    "        \n",
    "        # DÃ¹ng Gemini Ä‘á»ƒ check topic similarity\n",
    "        generation_config = genai.GenerationConfig(\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=40,\n",
    "            max_output_tokens=10,\n",
    "        )\n",
    "        \n",
    "        model = genai.GenerativeModel(\n",
    "            model_name=\"gemma-3-27b-it\",\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        \n",
    "        prompt = (\n",
    "            \"Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch chá»§ Ä‘á» y táº¿.\\n\"\n",
    "            \"Hai cÃ¢u há»i sau cÃ³ cÃ¹ng chá»§ Ä‘á»/bá»‡nh y táº¿ khÃ´ng?\\n\"\n",
    "            \"Tráº£ lá»i CHá»ˆ 'same' náº¿u cÃ¹ng chá»§ Ä‘á» hoáº·c 'different' náº¿u khÃ¡c chá»§ Ä‘á».\\n\\n\"\n",
    "            f\"CÃ¢u 1: {recent_question}\\n\"\n",
    "            f\"CÃ¢u 2: {new_question}\\n\\n\"\n",
    "            \"Tráº£ lá»i (same/different):\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            result = response.text.strip().lower()\n",
    "            \n",
    "            is_different_topic = \"different\" in result\n",
    "            \n",
    "            if is_different_topic:\n",
    "                print(f\"ğŸ”„ Topic change detected:\")\n",
    "                print(f\"  Old: {recent_question[:50]}...\")\n",
    "                print(f\"  New: {new_question[:50]}...\")\n",
    "            \n",
    "            return is_different_topic\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting topic change: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_rag_pipeline_with_memory(self, llm, retriever, prompt, memory):\n",
    "        \"\"\"Táº¡o ConversationalRetrievalChain\"\"\"\n",
    "        combine_docs_chain = StuffDocumentsChain(\n",
    "            llm_chain=LLMChain(llm=llm, prompt=prompt),\n",
    "            document_variable_name=\"context\"\n",
    "        )\n",
    "        \n",
    "        # Question generator vá»›i prompt cáº£i thiá»‡n\n",
    "        question_prompt = PromptTemplate(\n",
    "            template=(\n",
    "                \"<|im_start|>system\\n\"\n",
    "                \"Báº¡n lÃ  trá»£ lÃ½ viáº¿t láº¡i cÃ¢u há»i y táº¿.\\n\"\n",
    "                \"NHIá»†M Vá»¤:\\n\"\n",
    "                \"- Náº¾U cÃ¢u há»i má»›i tham chiáº¿u Ä‘áº¿n lá»‹ch sá»­ (dÃ¹ng 'Ä‘Ã³', 'nÃ y', 'váº­y', v.v.), \"\n",
    "                \"hÃ£y viáº¿t láº¡i thÃ nh cÃ¢u há»i Ä‘á»™c láº­p.\\n\"\n",
    "                \"- Náº¾U cÃ¢u há»i má»›i Äá»˜C Láº¬P, GIá»® NGUYÃŠN.\\n\"\n",
    "                \"QUY Táº®C:\\n\"\n",
    "                \"- KHÃ”NG Ä‘á»•i chá»§ ngá»¯ 'tÃ´i/mÃ¬nh/con tÃ´i' thÃ nh 'báº¡n'\\n\"\n",
    "                \"- KHÃ”NG thÃªm cÃ¢u há»i xÃ¡c nháº­n ('Ä‘Ãºng khÃ´ng?', 'pháº£i khÃ´ng?')\\n\"\n",
    "                \"- Giá»¯ nguyÃªn giá»ng Ä‘iá»‡u cá»§a ngÆ°á»i há»i\\n\"\n",
    "                \"<|im_end|>\\n\"\n",
    "                \"<|im_start|>user\\n\"\n",
    "                \"Lá»‹ch sá»­:\\n{chat_history}\\n\\n\"\n",
    "                \"CÃ¢u há»i má»›i: {question}\\n\\n\"\n",
    "                \"CÃ¢u há»i Ä‘á»™c láº­p:\\n\"\n",
    "                \"<|im_end|>\\n\"\n",
    "                \"<|im_start|>assistant\\n\"\n",
    "            ),\n",
    "            input_variables=[\"chat_history\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        question_generator = LLMChain(llm=llm, prompt=question_prompt)\n",
    "        \n",
    "        return ConversationalRetrievalChain(\n",
    "            retriever=retriever,\n",
    "            combine_docs_chain=combine_docs_chain,\n",
    "            question_generator=question_generator,\n",
    "            memory=memory,\n",
    "            return_source_documents=True,\n",
    "            output_key=\"answer\",\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"Load embedding model\"\"\"\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            google_api_key=GEMINI_API_KEY,\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    def load_retriever(self, retriever_name, embeddings):\n",
    "        \"\"\"Load retriever cho source\"\"\"\n",
    "        if retriever_name == \"wiki\":\n",
    "            retriever = RerankWikiRetriever(\n",
    "                vectorstore=WikipediaRetriever(\n",
    "                    lang=\"vi\",\n",
    "                    doc_content_chars_max=200,\n",
    "                    top_k_results=8,\n",
    "                    max_retries=3,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            client = QdrantClient(\n",
    "                url=QDRANT_URL,\n",
    "                api_key=QDRANT_API_KEY, \n",
    "                prefer_grpc=False,\n",
    "            )\n",
    "            db = Qdrant(\n",
    "                client=client,\n",
    "                embeddings=embeddings,\n",
    "                collection_name=QDRANT_COLLECTION_NAME\n",
    "            )\n",
    "            retriever = RerankRetriever(\n",
    "                vectorstore=db.as_retriever(\n",
    "                    search_kwargs={\n",
    "                        \"k\": 5,\n",
    "                        \"score_threshold\": 0.7,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        return retriever\n",
    "\n",
    "\n",
    "    def load_model_pipeline(self, max_new_tokens=200):\n",
    "        \"\"\"Load LLM vá»›i tham sá»‘ tá»‘i Æ°u cho medical QA - chá»‘ng repetition\"\"\"\n",
    "        llm = VLLM(\n",
    "            model=GENERATE_MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            max_new_tokens=max_new_tokens, \n",
    "            top_k=40,  \n",
    "            top_p=0.75,  \n",
    "            temperature=0.5,  \n",
    "            dtype=\"half\",\n",
    "            repetition_penalty=1.3,  \n",
    "            frequency_penalty=0.7,  \n",
    "            presence_penalty=0.6,  \n",
    "            stop=[\"<|im_end|>\", \"\\n<|im_start|>\", \"\\n\\n\\n\"], \n",
    "            tensor_parallel_size=2,\n",
    "        )\n",
    "        return llm\n",
    "\n",
    "    def load_prompt_template(self):\n",
    "        \"\"\"Load prompt template cho medical QA\"\"\"\n",
    "        query_template = (\n",
    "            \"<|im_start|>system\\n\"\n",
    "            \"Báº¡n lÃ  Vi Medical - trá»£ lÃ½ tÆ° váº¥n y táº¿ AI chuyÃªn nghiá»‡p.\\n\"\n",
    "            \"VAI TRÃ’: Báº¡n lÃ  ngÆ°á»i TÆ¯ Váº¤N y táº¿, ngÆ°á»i dÃ¹ng lÃ  Bá»†NH NHÃ‚N.\\n\"\n",
    "            \"NHIá»†M Vá»¤: PhÃ¢n tÃ­ch triá»‡u chá»©ng vÃ  Ä‘Æ°a ra lá»i khuyÃªn dá»±a trÃªn thÃ´ng tin tham kháº£o.\\n\"\n",
    "            \"QUY Táº®C:\\n\"\n",
    "            \"- Tráº£ lá»i ngáº¯n gá»n, dá»… hiá»ƒu báº±ng tiáº¿ng Viá»‡t\\n\"\n",
    "            \"- KHÃ”NG bao giá» nÃ³i 'tÃ´i bá»‹', 'tÃ´i cÃ³ triá»‡u chá»©ng'\\n\"\n",
    "            \"- LuÃ´n dÃ¹ng 'báº¡n', 'ngÆ°á»i bá»‡nh' khi Ä‘á» cáº­p Ä‘áº¿n triá»‡u chá»©ng\\n\"\n",
    "            \"- Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, khuyÃªn há» tham kháº£o bÃ¡c sÄ©\\n\"\n",
    "            \"<|im_end|>\\n\"\n",
    "            \"THÃ”NG TIN THAM KHáº¢O:\\n{context}\\n\"\n",
    "            \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\"\n",
    "            \"{question}\\n\"\n",
    "            \"<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        return PromptTemplate(\n",
    "            template=query_template, \n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:52:58.022170Z",
     "iopub.status.busy": "2026-01-06T19:52:58.021917Z",
     "iopub.status.idle": "2026-01-06T19:53:55.909431Z",
     "shell.execute_reply": "2026-01-06T19:53:55.908582Z",
     "shell.execute_reply.started": "2026-01-06T19:52:58.022146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app = LLMServe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T19:53:55.910929Z",
     "iopub.status.busy": "2026-01-06T19:53:55.910583Z",
     "iopub.status.idle": "2026-01-06T19:53:56.316813Z",
     "shell.execute_reply": "2026-01-06T19:53:56.315866Z",
     "shell.execute_reply.started": "2026-01-06T19:53:55.910894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from fastapi import FastAPI, Body\n",
    "from pydantic import BaseModel\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pydantic models for request validation\n",
    "class LoadHistoryRequest(BaseModel):\n",
    "    session_id: str\n",
    "    source: str\n",
    "    chat_history: list\n",
    "\n",
    "class GenerateTitleRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "origins = [\"*\"]\n",
    "app_api = FastAPI()\n",
    "app_api.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app_api.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Vi-Medical API RAG\", \"status\": \"running\"}\n",
    "\n",
    "@app_api.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": app.pipe is not None,\n",
    "    }\n",
    "\n",
    "@app_api.get(\"/rag/{source}\")\n",
    "async def read_item(source: str, q: str | None = None, session_id: str | None = None):\n",
    "    \"\"\"\n",
    "    Main RAG endpoint\n",
    "    \n",
    "    Args:\n",
    "        source: \"vi-medical\" hoáº·c \"wiki\"\n",
    "        q: CÃ¢u há»i\n",
    "        session_id: ID cá»§a session/chat\n",
    "    \"\"\"\n",
    "    if not q:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"No query provided.\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if not session_id:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"No session_id provided.\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if not q.strip():\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Query is empty.\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if len(q) > 500:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Query too long (max 500 characters).\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if source not in [\"vi-medical\", \"wiki\"]:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Invalid source. Use 'vi-medical' or 'wiki'.\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Processing query: session={session_id}, source={source}, q={q[:50]}...\")\n",
    "        \n",
    "        # Classify question\n",
    "        label = classify_question_with_gemini(q)\n",
    "        logger.info(f\"Question classified as: {label}\")\n",
    "        \n",
    "        # Handle greetings\n",
    "        if \"ChÃ o há»i\" in label:\n",
    "            return JSONResponse(content=jsonable_encoder({\n",
    "                \"result\": \"Xin chÃ o! TÃ´i lÃ  Vi-Medical, trá»£ lÃ½ y táº¿ AI. TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?\", \n",
    "                \"source_documents\": []\n",
    "            }))\n",
    "        \n",
    "        # Handle irrelevant questions\n",
    "        if \"KhÃ´ng liÃªn quan\" in label:\n",
    "            return JSONResponse(content=jsonable_encoder({\n",
    "                \"result\": \"CÃ¢u há»i cá»§a báº¡n khÃ´ng liÃªn quan Ä‘áº¿n y táº¿. TÃ´i chá»‰ cÃ³ thá»ƒ tÆ° váº¥n vá» sá»©c khá»e vÃ  y táº¿.\", \n",
    "                \"source_documents\": []\n",
    "            }))\n",
    "        \n",
    "        # Medical question - Main processing\n",
    "        logger.info(\"Processing medical question...\")\n",
    "        \n",
    "        # Get or create chain for session\n",
    "        chain = app.get_or_create_chain(session_id, source)\n",
    "        \n",
    "        # âœ… DETECT TOPIC CHANGE - Tá»± Ä‘á»™ng clear memory náº¿u chuyá»ƒn topic\n",
    "        session_key = app.get_session_key(session_id, source)\n",
    "        if app.detect_topic_change(q, session_key):\n",
    "            logger.info(\"ğŸ”„ Topic changed - clearing memory for cleaner context\")\n",
    "            app.sessions[session_key][\"memory\"].clear()\n",
    "        \n",
    "        # Call chain vá»›i cÃ¢u há»i Gá»C\n",
    "        # Memory sáº½ tá»± Ä‘á»™ng quáº£n lÃ½ context\n",
    "        response = chain({\n",
    "            \"question\": q  # CÃ¢u há»i Gá»C, khÃ´ng viáº¿t láº¡i trÆ°á»›c\n",
    "        })\n",
    "        \n",
    "        logger.info(\"Chain completed successfully\")\n",
    "        \n",
    "        answer = response.get(\"answer\", \"\")\n",
    "        source_documents = response.get(\"source_documents\", [])\n",
    "                \n",
    "        # Format sources\n",
    "        sources = []\n",
    "        if source_documents:\n",
    "            try:\n",
    "                sources = [doc.to_json()[\"kwargs\"] for doc in source_documents]\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error formatting sources: {e}\")\n",
    "                sources = []\n",
    "        \n",
    "        # Add medical disclaimer for vi-medical source\n",
    "        if source == \"vi-medical\" and answer:\n",
    "            answer += (\n",
    "                \"\\n\\nâš ï¸ **LÆ°u Ã½ quan trá»ng**: ThÃ´ng tin trÃªn chá»‰ mang tÃ­nh cháº¥t tham kháº£o. \"\n",
    "                \"Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c cháº©n Ä‘oÃ¡n vÃ  Ä‘iá»u trá»‹ chÃ­nh xÃ¡c.\"\n",
    "            )\n",
    "        \n",
    "        # Build response\n",
    "        res = {\n",
    "            \"result\": answer,\n",
    "            \"source_documents\": sources,\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Request completed successfully\")\n",
    "        return JSONResponse(content=jsonable_encoder(res))\n",
    "   \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {str(e)}\", exc_info=True)\n",
    "        return JSONResponse(\n",
    "            content={\"error\": f\"Internal server error: {str(e)}\"}, \n",
    "            status_code=500\n",
    "        )\n",
    "\n",
    "@app_api.post(\"/clear/{source}\")\n",
    "async def clear_memory(source: str, session_id: str | None = None):\n",
    "    \"\"\"Clear memory cho session\"\"\"\n",
    "    if not session_id:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"No session_id provided.\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if source not in [\"vi-medical\", \"wiki\"]:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Invalid source\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        message = app.clear_session(session_id, source)\n",
    "        logger.info(message)\n",
    "        return {\"message\": message}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing: {e}\")\n",
    "        return JSONResponse(\n",
    "            content={\"error\": str(e)}, \n",
    "            status_code=500\n",
    "        )\n",
    "\n",
    "@app_api.post(\"/load-history\")\n",
    "async def load_history(request: LoadHistoryRequest):\n",
    "    \"\"\"\n",
    "    Load lá»‹ch sá»­ chat vÃ o memory khi user quay láº¡i chat cÅ©\n",
    "    \n",
    "    Body: {\n",
    "        \"session_id\": \"chat_123\",\n",
    "        \"source\": \"vi-medical\",\n",
    "        \"chat_history\": [\n",
    "            {\"question\": \"...\", \"answer\": \"...\"},\n",
    "            {\"question\": \"...\", \"answer\": \"...\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    session_id = request.session_id\n",
    "    source = request.source\n",
    "    chat_history = request.chat_history\n",
    "    \n",
    "    if not session_id:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"No session_id\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if source not in [\"vi-medical\", \"wiki\"]:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Invalid source\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        message = app.load_session_memory(session_id, source, chat_history)\n",
    "        logger.info(f\"Loaded history for session {session_id}\")\n",
    "        return {\"message\": message}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading history: {e}\")\n",
    "        return JSONResponse(\n",
    "            content={\"error\": str(e)}, \n",
    "            status_code=500\n",
    "        )\n",
    "\n",
    "@app_api.post(\"/generate-title\")\n",
    "async def generate_title(request: GenerateTitleRequest):\n",
    "    \"\"\"\n",
    "    Generate tiÃªu Ä‘á» ngáº¯n gá»n cho Ä‘oáº¡n chat tá»« cÃ¢u há»i Ä‘áº§u tiÃªn\n",
    "    \n",
    "    Body: {\n",
    "        \"question\": \"TÃ´i bá»‹ Ä‘au Ä‘áº§u kÃ©o dÃ i 3 ngÃ y, pháº£i lÃ m sao?\"\n",
    "    }\n",
    "    \n",
    "    Response: {\n",
    "        \"title\": \"Äau Ä‘áº§u kÃ©o dÃ i 3 ngÃ y\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    question = request.question\n",
    "    \n",
    "    if not question or not question.strip():\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Question is empty\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    if len(question) > 500:\n",
    "        return JSONResponse(\n",
    "            content={\"error\": \"Question too long\"}, \n",
    "            status_code=400\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        title = generate_chat_title_with_gemini(question)\n",
    "        logger.info(f\"Generated title: {title}\")\n",
    "        return {\"title\": title}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating title: {e}\")\n",
    "        return JSONResponse(\n",
    "            content={\"error\": str(e)}, \n",
    "            status_code=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-06T19:57:14.824Z",
     "iopub.execute_input": "2026-01-06T19:53:56.318201Z",
     "iopub.status.busy": "2026-01-06T19:53:56.317854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "ngrok.set_auth_token(NGROK_TOKEN)\n",
    "ngrok_tunnel = ngrok.connect(8000, domain=NGROK_STATIC_DOMAIN)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app_api, port=8000)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
